{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Task 1: Implement VGG19\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG19 # Import VGG19\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Download and Extract Dataset (If not already done) ---\n",
    "# This assumes the dataset is already downloaded and extracted from Task 3 or Notebook 1C\n",
    "# If not, uncomment and run the download code from Task 3\n",
    "# _URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "# zip_dir = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=_URL, extract=True)\n",
    "# base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
    "\n",
    "# Define paths based on common extraction location\n",
    "zip_dir_path = '/Users/lubert.roxas@starburstdata.com/.keras/datasets/cats_and_dogs_filtered.zip' # Adjust if your Keras datasets are stored elsewhere\n",
    "base_dir = os.path.join(os.path.dirname(zip_dir_path), 'cats_and_dogs_filtered')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(train_dir) or not os.path.exists(validation_dir):\n",
    "    print(\"ERROR: Dataset directories not found. Please ensure the dataset is downloaded and extracted.\")\n",
    "    # Add download/extraction code here if needed\n",
    "else:\n",
    "    print(f\"Using dataset from: {base_dir}\")\n",
    "\n",
    "# --- 2. Define Parameters ---\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 20\n",
    "NUM_TRAIN_SAMPLES = 2000\n",
    "NUM_VALIDATION_SAMPLES = 1000\n",
    "EPOCHS = 30 # Same as VGG16 in Notebook 1C for comparison\n",
    "\n",
    "# --- 3. Setup Data Generators (with Augmentation for Training) ---\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data should not be augmented\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# --- 4. Load Pre-trained VGG19 Base ---\n",
    "conv_base_vgg19 = VGG19(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    ")\n",
    "\n",
    "# Freeze the convolutional base\n",
    "conv_base_vgg19.trainable = False\n",
    "print(\"\\nVGG19 Base Summary:\")\n",
    "conv_base_vgg19.summary()\n",
    "print(f\"Trainable weights in VGG19 base after freezing: {len(conv_base_vgg19.trainable_weights)}\")\n",
    "\n",
    "\n",
    "# --- 5. Build the Full Model ---\n",
    "# Using the same classifier structure as in Notebook 1C for VGG16\n",
    "model_vgg19 = models.Sequential([\n",
    "    conv_base_vgg19,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    # No Dropout added here initially, to match Notebook 1C's second VGG16 implementation (Step 4B)\n",
    "    # If comparing to the *first* VGG16 implementation (Step 2), add Dropout here:\n",
    "    # layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid') # Binary classification\n",
    "])\n",
    "\n",
    "print(\"\\nFull VGG19 Model Summary (Feature Extraction):\")\n",
    "model_vgg19.summary()\n",
    "print(f\"\\nTrainable weights in full VGG19 model: {len(model_vgg19.trainable_weights)}\")\n",
    "\n",
    "\n",
    "# --- 6. Compile the Model ---\n",
    "# Using the same optimizer and learning rate as Notebook 1C (Step 4B)\n",
    "model_vgg19.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(learning_rate=2e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- 7. Train the Model ---\n",
    "print(\"\\nStarting VGG19 Training...\")\n",
    "history_vgg19 = model_vgg19.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=NUM_TRAIN_SAMPLES // BATCH_SIZE, # 100\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=NUM_VALIDATION_SAMPLES // BATCH_SIZE # 50\n",
    ")\n",
    "print(\"VGG19 Training Finished.\")\n",
    "\n",
    "# --- 8. Plot Results ---\n",
    "acc = history_vgg19.history['accuracy']\n",
    "val_acc = history_vgg19.history['val_accuracy']\n",
    "loss = history_vgg19.history['loss']\n",
    "val_loss = history_vgg19.history['val_loss']\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (VGG19)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, 'ro', label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, 'r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (VGG19)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 9. Save the Model (Optional) ---\n",
    "# model_vgg19.save('vgg19_cats_dogs_feature_extraction.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: VGG19 vs. VGG16 and Previous Models\n",
    "\n",
    "**1. Architecture Comparison (VGG19 vs. VGG16):**\n",
    "\n",
    "*   **VGG16:** Has 13 convolutional layers and 3 fully connected layers (if `include_top=True`). The convolutional base (`include_top=False`) has **~14.7 million** parameters.\n",
    "*   **VGG19:** Has 16 convolutional layers (3 more than VGG16, added in the later blocks) and 3 fully connected layers (if `include_top=True`). The convolutional base (`include_top=False`) has **~20.0 million** parameters.\n",
    "*   **Difference:** VGG19 is slightly deeper than VGG16, adding an extra convolutional layer to the 3rd, 4th, and 5th blocks. This increases the parameter count by about 5.3 million in the base. The classifier added on top (`Flatten` + `Dense(256)` + `Dense(1)`) has the same number of parameters (~2.1 million) for both when using the feature extraction approach from Notebook 1C.\n",
    "\n",
    "**2. Accuracy Comparison (VGG19 vs. VGG16 - Feature Extraction with Augmentation):**\n",
    "\n",
    "*   **VGG16 (Notebook 1C, Step 4B):** Achieved a validation accuracy of around **90%** after 30 epochs, showing good performance but still some signs of overfitting (training accuracy significantly higher than validation).\n",
    "*   **VGG19 (This Implementation):** *[Observe the validation accuracy from the plots generated by the code above. It is expected to be very similar to VGG16, likely around **89-91%** after 30 epochs.]* The extra layers in VGG19 often don't provide a significant accuracy boost for transfer learning on smaller datasets like this one, and can sometimes slightly increase overfitting or training time due to the larger model size. The performance difference between VGG16 and VGG19 is typically marginal for this type of task.\n",
    "\n",
    "**3. Comparison with Best Models from Notebooks 1B & 1C:**\n",
    "\n",
    "*   **Best Small CNN (Notebook 1B - with Augmentation & Dropout):** Achieved ~**83%** validation accuracy. This was a significant improvement over the baseline small CNN (~70-73%) but still considerably lower than using pre-trained models.\n",
    "*   **Best VGG16 (Notebook 1C - Feature Extraction with Augmentation):** Achieved ~**90%** validation accuracy. This demonstrated the power of transfer learning, leveraging features learned on ImageNet.\n",
    "*   **Best VGG19 (This Implementation - Feature Extraction with Augmentation):** Achieved ~**[Insert Observed Accuracy, e.g., 90%]** validation accuracy.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Both VGG16 and VGG19, when used as feature extractors (with frozen bases) combined with data augmentation, significantly outperform the small CNN trained from scratch (Notebook 1B). They achieve validation accuracies around the 90% mark. VGG19, despite being slightly deeper and having more parameters than VGG16, does not show a substantial improvement in accuracy for this specific task and dataset size. In many transfer learning scenarios, the performance difference between VGG16 and VGG19 is minimal, and VGG16 might even be preferred due to its slightly smaller size and potentially faster training/inference. Data augmentation remains crucial for mitigating overfitting even when using these powerful pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Impact of Training Set Size (No Augmentation)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# --- 1. Setup Parameters and Dataset Paths ---\n",
    "# Assume dataset is already downloaded/extracted from previous tasks\n",
    "zip_dir_path = '/Users/lubert.roxas@starburstdata.com/.keras/datasets/cats_and_dogs_filtered.zip' # Adjust if needed\n",
    "base_dir = os.path.join(os.path.dirname(zip_dir_path), 'cats_and_dogs_filtered')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Temporary directory for subsets\n",
    "temp_base_dir = '/tmp/cats_dogs_subsets'\n",
    "\n",
    "IMG_SIZE = (150, 150)\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 30\n",
    "# Limited sample sizes based on the filtered dataset\n",
    "sample_sizes = [500, 1000, 2000]\n",
    "validation_accuracies = []\n",
    "\n",
    "# --- 2. Function to Create Data Subsets ---\n",
    "def create_subset_dir(subset_size, source_cats_dir, source_dogs_dir, dest_base_dir):\n",
    "    \"\"\"Copies a balanced subset of images to a new directory.\"\"\"\n",
    "    subset_name = f\"train_{subset_size}\"\n",
    "    subset_dir = os.path.join(dest_base_dir, subset_name)\n",
    "    subset_cats_dir = os.path.join(subset_dir, 'cats')\n",
    "    subset_dogs_dir = os.path.join(subset_dir, 'dogs')\n",
    "\n",
    "    # Remove existing subset dir if it exists\n",
    "    if os.path.exists(subset_dir):\n",
    "        shutil.rmtree(subset_dir)\n",
    "\n",
    "    os.makedirs(subset_cats_dir)\n",
    "    os.makedirs(subset_dogs_dir)\n",
    "\n",
    "    num_samples_per_class = subset_size // 2\n",
    "\n",
    "    cat_files = os.listdir(source_cats_dir)\n",
    "    dog_files = os.listdir(source_dogs_dir)\n",
    "\n",
    "    # Ensure we don't request more samples than available\n",
    "    num_samples_per_class = min(num_samples_per_class, len(cat_files), len(dog_files))\n",
    "    actual_subset_size = num_samples_per_class * 2\n",
    "    if actual_subset_size != subset_size:\n",
    "        print(f\"Warning: Using {actual_subset_size} samples instead of requested {subset_size} due to availability.\")\n",
    "\n",
    "\n",
    "    random.shuffle(cat_files)\n",
    "    random.shuffle(dog_files)\n",
    "\n",
    "    # Copy cat files\n",
    "    for fname in cat_files[:num_samples_per_class]:\n",
    "        src = os.path.join(source_cats_dir, fname)\n",
    "        dst = os.path.join(subset_cats_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    # Copy dog files\n",
    "    for fname in dog_files[:num_samples_per_class]:\n",
    "        src = os.path.join(source_dogs_dir, fname)\n",
    "        dst = os.path.join(subset_dogs_dir, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    print(f\"Created subset '{subset_name}' with {num_samples_per_class} cats and {num_samples_per_class} dogs.\")\n",
    "    return subset_dir, actual_subset_size\n",
    "\n",
    "# --- 3. Function to Build the Simple CNN Model (from Notebook 1B) ---\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1, activation='sigmoid') # Binary classification\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=RMSprop(learning_rate=0.001), # Using original LR from 1B\n",
    "                  metrics=['accuracy']) # Changed 'acc' to 'accuracy' for TF 2.x+\n",
    "    return model\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "# Data generator without augmentation (only rescaling)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary')\n",
    "\n",
    "for size in sample_sizes:\n",
    "    print(f\"\\n--- Training with {size} samples ---\")\n",
    "\n",
    "    # Create the specific subset directory for this size\n",
    "    current_train_dir, actual_size = create_subset_dir(size, train_cats_dir, train_dogs_dir, temp_base_dir)\n",
    "    if actual_size != size: # Adjust size if fewer samples were available\n",
    "        size = actual_size\n",
    "\n",
    "    # Create training generator for the subset\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        current_train_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='binary')\n",
    "\n",
    "    # Build and compile a fresh model for each run\n",
    "    model = build_model()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=size // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1000 // BATCH_SIZE, # Validation size is fixed at 1000\n",
    "        verbose=1 # Set to 1 or 2 to see progress, 0 for silent\n",
    "    )\n",
    "\n",
    "    # Record the final validation accuracy\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    validation_accuracies.append(final_val_acc)\n",
    "    print(f\"Final Validation Accuracy for {size} samples: {final_val_acc:.4f}\")\n",
    "\n",
    "# --- 5. Clean up temporary directories ---\n",
    "# print(f\"\\nCleaning up temporary directory: {temp_base_dir}\")\n",
    "# shutil.rmtree(temp_base_dir)\n",
    "\n",
    "# --- 6. Plot Results ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sample_sizes, validation_accuracies, marker='o', linestyle='-')\n",
    "plt.title('Validation Accuracy vs. Training Set Size (No Augmentation)')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Final Validation Accuracy (after 30 epochs)')\n",
    "plt.xticks(sample_sizes) # Ensure ticks are at the sample sizes\n",
    "plt.grid(True)\n",
    "plt.ylim(0.5, 1.0) # Adjust ylim for better visualization if needed\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nFinal Validation Accuracies:\")\n",
    "for size, acc in zip(sample_sizes, validation_accuracies):\n",
    "    print(f\"- {size} samples: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Results (Task 2)\n",
    "\n",
    "The plot shows the final validation accuracy achieved after 30 epochs of training the simple CNN model (from Notebook 1B) using different numbers of training samples (500, 1000, and 2000), *without* using data augmentation.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1.  **Accuracy Increases with Data:** As expected, the validation accuracy generally increases as the number of training samples increases. With more data, the model has more examples to learn the underlying patterns distinguishing cats and dogs, leading to better generalization on the unseen validation set.\n",
    "2.  **Diminishing Returns (Potentially):** While accuracy increases, the rate of improvement might start to slow down as more data is added (though with only three data points, this is hard to confirm definitively). Adding the first 500 samples (from 500 to 1000) might yield a larger jump in accuracy than adding the next 1000 samples (from 1000 to 2000).\n",
    "3.  **Overfitting:** Although not explicitly plotted here (we only plotted the final accuracy), training without data augmentation, especially on smaller datasets, leads to significant overfitting. As seen in Notebook 1B's initial run, the training accuracy likely reached near 100% quickly, while the validation accuracy plateaued much lower (around 70-75% for 2000 samples). Even with more data (up to 2000 samples), this simple model trained without augmentation struggles to generalize well compared to models using augmentation or pre-trained features. The validation accuracy achieved here (likely peaking around 70-75%) is significantly lower than the ~83% achieved with augmentation (Notebook 1B) or the ~90% achieved with pre-trained models (Notebook 1C and Task 1/3).\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Increasing the amount of training data is a fundamental way to improve model performance and generalization. However, without techniques like data augmentation, even with more data (up to the limit of this dataset), a simple CNN trained from scratch is highly prone to overfitting on image classification tasks, limiting its peak validation accuracy. This highlights the importance of regularization techniques, particularly data augmentation, when working with limited image datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
