{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6ogf1a0sRy7"
   },
   "source": [
    "# (PyTorch) Notebook 1A. Convolutional Neural Networks (CNN)\n",
    "---\n",
    "Organized and prepared by Christopher Monterola, updated by Kenneth Co.\n",
    "\n",
    "This notebook was conceptualized, organized, and primarily prepared for the **Machine Learning** courses.\n",
    "\n",
    "### This notebook uses the following references:\n",
    "1. Python Machine Learning, Second Edition, Sebastian Raschka and Vahid Mirjalili, Packt Publishing Ltd. Birmingham B3 2PB, UK Sept 2017.\n",
    "2. Hands-On Machine Learning with Scikit-Learn and TensorFlow, Aur√©lien G√©ron, O'Reilly 2017.\n",
    "3. Deep Learning with Python, Francois Chollet, Manning New York 2018.\n",
    "\n",
    "In this notebook, we learn about Convolutional Neural Networks (CNNs), and how we can implement CNNs in TensorFlow/Keras. We'll also take an interesting journey as we apply this type of deep neural network architecture to image classification. We'll start by discussing the basic building blocks of CNNs, using a bottom up approach. Then we'll take a deeper dive into the CNN architecture and how to implement deep CNNs in TensorFlow. Along the way we'll be covering the following topics:   \n",
    "\n",
    "- Understanding convolution operations in one and two dimensions\n",
    "- Learning about the building blocks of CNN architectures\n",
    "- Implementing deep convolutional neural networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7veB3BtW0Q8o"
   },
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:37.589023Z",
     "iopub.status.busy": "2025-01-14T02:50:37.588824Z",
     "iopub.status.idle": "2025-01-14T02:50:37.593323Z",
     "shell.execute_reply": "2025-01-14T02:50:37.592535Z",
     "shell.execute_reply.started": "2025-01-14T02:50:37.589005Z"
    },
    "executionInfo": {
     "elapsed": 2381,
     "status": "ok",
     "timestamp": 1737201151345,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "dX3CGlLe0Q8o",
    "outputId": "c9a686c9-84d5-4e34-ac86-f8e3183ee43d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# img_dir = '/content/drive/MyDrive/COSCI224 Machine Learning 3 Notebooks/images/'\n",
    "img_dir = 'images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyH4hXEpsRy_"
   },
   "source": [
    "# 1. General Introduction: Building blocks of convolutional neural networks\n",
    "---\n",
    "\n",
    "Convolutional neural networks, or CNNs, are a family of models that were inspired by how the **visual cortex of human brain** works when recognizing objects. The development of CNNs goes back to the 1990's, when Yann LeCun and his colleagues proposed a novel neural network architecture for classifying handwritten digits from images (Handwritten Digit Recognition with a Back-Propagation Network, Y LeCun, and others, 1989, published at Neural Information Processing Systems(NIPS) conference). Due to the outstanding performance of CNNs for image classification tasks, they have gained a lot of attention and this led to tremendous improvements in machine learning and computer vision applications.      \n",
    "\n",
    "In the following sections, we next see how CNNs are used as feature extraction engines, and then we'll delve into the theoretical definition of convolution and computing convolution in one and two dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc21Lv0e0Q8p"
   },
   "source": [
    "## Understanding CNNs and learning feature hierarchies\n",
    "\n",
    "Successfully **extracting salient (relevant) features** is key to the performance of any machine learning algorithm, of course, and traditional machine learning models rely on input features that may come from a domain expert, or are based on computational feature extraction techniques. Neural networks are able to automatically learn the features from raw data that are most useful for a particular task. For this reason, it's common to consider a neural network as a feature extraction engine: the **early layers** (those right after the input layer) extract **low level features**.     \n",
    "\n",
    "Multilayer neural networks, and in particular, deep convolutional neural networks, construct a so-called feature hierarchy by combining the low-level features in a layer-wise fashion to form high-level features. For example, if we're dealing with images, then low-level features, such as edges and blobs, are extracted from the earlier layers, which are combined together to form high-level features ‚Äì as object shapes like a building, a car, or a dog.    \n",
    "\n",
    "As you can see in the following image, a CNN computes feature maps from an input image, where each element comes from a local patch of pixels in the input image:\n",
    "\n",
    "<img width=\"919\" alt=\"CNN_dog\" src=\"https://user-images.githubusercontent.com/25600601/134186781-c3eb2e95-fcce-4f65-8f86-60317bfa11d7.png\">\n",
    "\n",
    "perform very well for image-related tasks, and that's largely due to two important ideas:     \n",
    "\n",
    "- **Sparse-connectivity:** A single element in the feature map is connected to only a small patch of pixels. (This is very different from connecting to the whole input image, in the case of perceptrons. You may find it useful to look back and compare how we implemented a fully connected network that connected to the whole image.)    \n",
    "\n",
    "- **Parameter-sharing:** The same weights are used for different patches of the input image. As a direct consequence of these two ideas, the number of weights (parameters) in the network decreases dramatically, and we see an improvement in the ability to capture salient features. Intuitively, it makes sense that *nearby pixels are probably more relevant to each other* than pixels that are far away from each other.\n",
    "\n",
    "Typically, CNNs are composed of *several Convolutional (conv)* layers and *subsampling (also known as Pooling (P))* layers that are followed by one or more Fully Connected (FC) layers at the end. The fully connected layers are essentially a multilayer perceptron, where every input unit $i$ is connected to every output unit $j$ with weight $w_{ij}$ (which we learned previously).\n",
    "\n",
    "In the following sections, we'll study convolutional and pooling layers in more detail and see how they work. To understand how convolution operations work, let's start with a convolution in one dimension before working through the typical two-dimensional cases as applications for two-dimensional images later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe57mwlJsRzA"
   },
   "source": [
    "# 2. Performing discrete convolutions\n",
    "---\n",
    "\n",
    "A discrete convolution (or simply convolution) is a fundamental operation in a CNN. Therefore, it's important to understand how this operation works. In this section, we'll learn the mathematical definition and discuss some of the naive algorithms to compute convolutions of two one-dimensional vectors or two twodimensional\n",
    "matrices.  \n",
    "\n",
    "Please note that this description is solely for understanding how a convolution works. Indeed, much more efficient implementations of convolutional operations already exist in packages such as TensorFlow, as we will implement eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHmcQYQJsRzB"
   },
   "source": [
    "## 2.1 Performing a discrete convolution in one dimension\n",
    "\n",
    "Let's start with some basic definitions and notations we are going to use. A discrete convolution for two one-dimensional vectors $\\mathbf{x}$ and $\\mathbf{w}$ is denoted by $\\mathbf{y}= \\mathbf{x}*\\mathbf{w}$ , in which vector $\\mathbf{x}$ is our input (sometimes called signal) and $\\mathbf{w}$ is called the filter or kernel. A discrete convolution is mathematically defined as follows:   \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}= \\mathbf{x} * \\mathbf{w} = \\sum_{k=-\\infty}^{k=+\\infty} \\mathbf{x} [i-k] \\mathbf{w}[k]\n",
    "\\end{equation}\n",
    "\n",
    "Here, the brackets [...] are used to denote the indexing for vector elements. The index $i$ runs through each element of the output vector $\\mathbf{y}$. There are two odd things in the preceding formula that we need to clarify: $‚àí\\infty$ to $+\\infty$ indices and negative indexing for $\\mathbf{x}$.\n",
    "\n",
    "The **first issue** where the **sum runs through indices from $‚àí\\infty$ to $+\\infty$** seems odd mainly because in machine learning applications, we always deal with finite feature vectors. For example, if $\\mathbf{x}$ has 10 features with indices 0,1,2,‚Ä¶,8,9, then indices $‚àí\\infty:-1$ and $10 : +\\infty$ are out of bounds for $\\mathbf{x}$. Therefore, to correctly compute the summation shown in the preceding formula, it is assumed that $\\mathbf{x}$ and $\\mathbf{w}$ are filled with zeros. This will result in an output vector $\\mathbf{y}$ that also has infinite size with lots of zeros as well. Since this is not useful in practical situations, $\\mathbf{x}$ is padded only with a\n",
    "finite number of zeros.   \n",
    "\n",
    "This process is called **zero-padding** or simply **padding**. Here, the number of zeros padded on each side is denoted by *p*. An example padding of a one-dimensional vector $\\mathbf{x}$ is shown in the following figure:\n",
    "\n",
    "<img width=\"670\" alt=\"zero-padding\" src=\"https://user-images.githubusercontent.com/25600601/134187112-666edbe5-6e2f-4982-b01b-721ae3f53969.png\">\n",
    "\n",
    "\n",
    "Let's assume that the original input $\\mathbf{x}$ and filter $\\mathbf{w}$ have $n$ and $m$ elements, respectively, where $m \\leq n$. Therefore, the padded vector $\\mathbf{x}^p$  has size $n + 2p$. Then, the practical formula for computing a discrete convolution will change to the following:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y}= \\mathbf{x} * \\mathbf{w} \\Rightarrow \\mathbf{y}[i]= \\sum_{k=0}^{k=m-1} \\mathbf{x}^p [i+m-k] \\mathbf{w}[k]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now that we have solved the infinite index issue, the second issue is indexing $\\mathbf{x}$ with $i + m - k$. The important point to notice here is that $\\mathbf{x}$ and $\\mathbf{w}$ are indexed in different directions in this summation. For this reason, we can flip one of those vectors, $\\mathbf{x}$ or $\\mathbf{w}$, after they are padded. Then, we can simply compute their dot product.    \n",
    "\n",
    "Let's assume we flip the filter $\\mathbf{w}$ to get the rotated filter $\\mathbf{w}^r$ . Then, the dot product $\\mathbf{x}[i : i + m]$ $\\cdot$ $\\mathbf{w}^r$ is computed to get one element $\\mathbf{y}[i]$, where $\\mathbf{x}[i : i + m]$ is a patch of $\\mathbf{x}$ with size $m$.\n",
    "\n",
    "This operation is repeated like in a sliding window approach to get all the output elements. The following figure provides an example with $\\mathbf{x} = (3,2,1,7,1,2,5,4)$ and $\\mathbf{w}$ = ($\\frac{1}{2}, \\frac{3}{4}, 1, \\frac{1}{4}$) so that the first three output elements are computed:\n",
    "\n",
    "<img width=\"738\" alt=\"padding_illustration\" src=\"https://user-images.githubusercontent.com/25600601/134187244-781c5e5e-064a-4194-8ea8-be13285e41fb.png\">\n",
    "\n",
    "You can see in the preceding example that the padding size is zero ($p = 0$). Notice that the rotated filter wr is shifted by two cells each time we shift. This shift is another hyperparameter of a convolution, the stride $s$. In this example, the **stride is two**, $s = 2$. Note that the stride has to be a positive number smaller than the size of the input vector $\\mathbf{x}$ (pause and answer why?). We'll talk more about padding and strides in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyvcSQHBsRzC"
   },
   "source": [
    "### The effect of zero-padding in a convolution\n",
    "\n",
    "So far here, we've used zero-padding in convolutions to compute finite-sized output vectors. Technically, padding can be applied with any $p\\ge0$. Depending on the choice $p$, boundary cells may be treated differently than the cells located in the middle of $\\mathbf{x}$.   \n",
    "\n",
    "\n",
    "Now consider an example where $n = 5$ (number of elements in $\\mathbf{x}$), $m = 3$ (number of elements in $\\mathbf{w}$. Then, $p = 0$, $\\mathbf{x}[0]$ is only used in computing one output element (for instance, $\\mathbf{y}[0]$), while $\\mathbf{x}[1]$ is used in the computation of two output elements (for   instance, $\\mathbf{y}[0]$ and $\\mathbf{y}[1]$). So, you can see that this different treatment of elements of $\\mathbf{x}$ can artificially put more emphasis on the middle element, $\\mathbf{x}[2]$ , since it has appeared in most computations. We can avoid this issue if we choose $p = 2$, in which case, each element of $\\mathbf{x}$ will be involved in\n",
    "computing three elements of $\\mathbf{y}$.   \n",
    "\n",
    "\n",
    "Furthermore, the size of the output $\\mathbf{y}$ also depends on the choice of the padding strategy we use. There are three modes of padding that are commonly used in practice: **full**, **same**, and **valid**:\n",
    "\n",
    "‚Ä¢ In the **full mode**, the padding parameter $p$ is set to $p = m - 1$. Full padding increases the dimensions of the output; thus, it is rarely used in convolutional neural network architectures. An increasing dimension has problems with control of architectures and generalization (as it tends to overfit).\n",
    "\n",
    "‚Ä¢ **Same padding** is usually used if you want to have the size of the output ($\\mathbf{y}$) the same as the input vector $\\mathbf{x}$. In this case, the padding parameter $p$ is computed according to the filter size, along with the requirement that the input size and output size are the same.\n",
    "\n",
    "‚Ä¢ Finally, computing a convolution in the **valid mode** refers to the case where $p = 0$ (no padding).\n",
    "\n",
    "The following figure illustrates the three different padding modes for a simple $5 \\times 5$ pixel input with a kernel size of $3 \\times 3$ and a stride of $1$:\n",
    "\n",
    "<img width=\"807\" alt=\"different_padding\" src=\"https://user-images.githubusercontent.com/25600601/134187465-1f160b8d-cab8-4987-83fa-4a59d965f922.png\">\n",
    "\n",
    "**IMPORTANT NOTE**: The most commonly used padding mode in convolutional neural networks is **same padding**. One of its advantages over the other padding modes is that same padding preserves the height and width of the input images or tensors, which makes designing a network architecture more convenient.  \n",
    "\n",
    "One big disadvantage of the valid padding versus full and same padding, for example, is that the volume of the tensors would decrease substantially in neural networks with many layers, which can be detrimental to the network performance.\n",
    "\n",
    "In practice, it is recommended that you preserve the spatial size using same padding for the convolutional layers and  decrease the spatial size via pooling layers instead (*kapit lang/hold on, we will learn this in Section 3 below!*). As for the full padding, its size results in an output larger than the input size. Full padding is usually used in signal processing applications where it is important to minimize  boundary effects. **However, in deep learning context, boundary effect is not usually an issue**, so we rarely see full padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTaB4S5QsRzC"
   },
   "source": [
    "### Determining the size of the convolution output\n",
    "\n",
    "The output size of a convolution is determined by the total  number of times that we shift the filter  $\\mathbf{w}$  along the input vector.Again, if the input vector $\\mathbf{x}$ has size\n",
    "$n$ and the filter  $\\mathbf{w}$ is of size $m$. Then, the size of the output resulting from  $\\mathbf{x} *  \\mathbf{w}$  with padding $p$ and stride $s$ is determined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "o=floor (\\frac{n + 2p - m}{s}) + 1\n",
    "\\end{equation}\n",
    "\n",
    "Here, the floor operation returns the largest integer that is equal or smaller to the input, e.g. floor (1.43) =1\n",
    "\n",
    "\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "  ‚Ä¢ Compute the output size for an input vector of size 10 with a convolution kernel of size 5, padding 2, and stride 1:\n",
    "    \n",
    "\\begin{equation}\n",
    "n=10, m=5, p=2, s=1, \\Rightarrow o= floor(\\frac{10+ 2 \\times 2 -5}{1}) + 1 = 10\n",
    "\\end{equation}\n",
    "\n",
    "(Note that in this case, the output size turns out to be the same as the input; therefore, we conclude this as **mode='same' **)\n",
    "\n",
    "See illustration below if you want to trace it perfectly:\n",
    "\n",
    "![CNN_Convolution_Example1](https://user-images.githubusercontent.com/25600601/134187810-ed86251d-ddef-45d0-ae1d-62ff80c7d56b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lba3o8gW0Q8q"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## ‚ö†Ô∏è Checkpoint ‚ö†Ô∏è\n",
    "\n",
    "In the next 5 minutes, make sure everyone in your LT follows the discussion of padding and stride.\n",
    "</div>\n",
    "\n",
    "**Here is a great tool to visualize how it works: https://ezyang.github.io/convolution-visualizer/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPYSOiRNsRzE"
   },
   "source": [
    "### Example 1. One dimensional convolution\n",
    "In order to learn how to compute convolutions in one dimension, a naive implementation is shown in the following code block, and the results are compared with the *numpy.convolve* function. The code is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:37.594292Z",
     "iopub.status.busy": "2025-01-14T02:50:37.594085Z",
     "iopub.status.idle": "2025-01-14T02:50:37.682933Z",
     "shell.execute_reply": "2025-01-14T02:50:37.681324Z",
     "shell.execute_reply.started": "2025-01-14T02:50:37.594273Z"
    },
    "id": "NcQmClLXsRzF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def conv1d(x, w, p, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s),s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:37.683686Z",
     "iopub.status.busy": "2025-01-14T02:50:37.683429Z",
     "iopub.status.idle": "2025-01-14T02:50:37.690574Z",
     "shell.execute_reply": "2025-01-14T02:50:37.689516Z",
     "shell.execute_reply.started": "2025-01-14T02:50:37.683666Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737201151345,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "s2vqzcEBsRzG",
    "outputId": "d31d996b-9db4-44e8-86dd-deabc28dab99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d Implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n"
     ]
    }
   ],
   "source": [
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d Implementation:', conv1d(x, w, p=2, s=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:37.691344Z",
     "iopub.status.busy": "2025-01-14T02:50:37.691150Z",
     "iopub.status.idle": "2025-01-14T02:50:37.696897Z",
     "shell.execute_reply": "2025-01-14T02:50:37.696085Z",
     "shell.execute_reply.started": "2025-01-14T02:50:37.691327Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737201151345,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "wqPiHfAPsRzH",
    "outputId": "2ef46985-e3c2-4a76-a4ac-b143be15b036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "print('Numpy Results:', np.convolve(x, w, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErFTKp3FsRzH"
   },
   "source": [
    "## 2.2 Performing a discrete convolution in 2D\n",
    "\n",
    "The concepts you learned in the previous sections are easily extendible to two dimensions. When we deal with two dimensional input, such as a matrix $\\mathbf{X}_{n1√ón2}$ and the filter matrix $\\mathbf{W}_{m1√óm2}$, where $m_1 \\leq n_1$ and $m_2 \\leq n_2$ , then the matrix $\\mathbf{Y}$ = $\\mathbf{X} * \\mathbf{W}$ is the result of 2D convolution of $\\mathbf{X}$ with $\\mathbf{W}$. This is mathematically defined as follows:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y}= \\mathbf{X} * \\mathbf{W} \\Rightarrow \\mathbf{Y}[i, j]= \\sum_{k_1=-\\infty}^{k_2=\\infty}\\sum_{k_2=-\\infty}^{\\infty} \\mathbf{X} [i-k_1, j-k_2] \\mathbf{W}[k_1, k_2]\n",
    "\\end{equation}\n",
    "\n",
    "Notice that if you omit one of the dimensions, the remaining formula is exactly the same as the one we used previously to compute the convolution in 1D. In fact, all the previously mentioned techniques, such as zero-padding, rotating the filter matrix, and the use of strides, are also applicable to 2D convolutions, provided that they are extended to both the dimensions independently. The following example illustrates the computation of a 2D convolution between an input matrix $\\mathbf{X}_{3√ó3}$, a kernel matrix $\\mathbf{W}_{3√ó3}$, padding $p = (1, 1)$, and stride $s = (2, 2)$. According to the specified padding, one layer of zeros are padded on each side of the input matrix, which results in the padded matrix $\\mathbf{X}_{5√ó5}^{padded}$, as follows:\n",
    "\n",
    "<img width=\"746\" alt=\"padded_2d\" src=\"https://user-images.githubusercontent.com/25600601/134188176-c6a412b6-c6bd-49a4-bbdd-508b285e0882.png\">\n",
    "\n",
    "With the preceding filter, the rotated filter will be:\n",
    "\n",
    "<img width=\"373\" alt=\"Wrotated\" src=\"https://user-images.githubusercontent.com/25600601/134188347-20df516d-0292-430a-91d5-bcc9eebc366e.png\">\n",
    "\n",
    "Note that this rotation is not the same as the transpose matrix. To get the rotated filter in NumPy, we can write W_rot=W[::-1,::-1]. Next, we can shift the rotated filter matrix along the padded input matrix $\\mathbf{X}$ padded like a sliding window and compute the sum of the element-wise product, which is denoted by the dot operator in the\n",
    "following figure:\n",
    "\n",
    "<img width=\"900\" alt=\"2d_convolution\" src=\"https://user-images.githubusercontent.com/25600601/134188450-7da6a7f4-cea5-4b0d-80cd-34b56d3804ee.png\">\n",
    "\n",
    "The result will be the $2 \\times 2$ matrix $\\mathbf{Y}$.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edr969kgsRzH"
   },
   "source": [
    "### Example 2. Two-Dimensional (2D) Convolution\n",
    "\n",
    "Let's implement the 2D convolution according to the na√Øve algorithm described above. The *scipy.signal* package provides a way to compute 2D convolution via the\n",
    "*scipy.signal.convolve2d* function:\n",
    "\n",
    "good resource: https://www.saama.com/blog/different-kinds-convolutional-filters/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:37.697515Z",
     "iopub.status.busy": "2025-01-14T02:50:37.697331Z",
     "iopub.status.idle": "2025-01-14T02:50:38.051036Z",
     "shell.execute_reply": "2025-01-14T02:50:38.049971Z",
     "shell.execute_reply.started": "2025-01-14T02:50:37.697498Z"
    },
    "id": "hARdSzypsRzI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def conv2d(X, W, p=(0, 0), s=(1, 1)):\n",
    "    W_rot = np.array(W)[::-1,::-1]\n",
    "    X_orig = np.array(X)\n",
    "    n1 = X_orig.shape[0] + 2*p[0]\n",
    "    n2 = X_orig.shape[1] + 2*p[1]\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0]:p[0]+X_orig.shape[0], p[1]:p[1]+X_orig.shape[1]] = X_orig\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - W_rot.shape[0])/s[0])+1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1])/s[1])+1, s[1]):\n",
    "            X_sub = X_padded[i:i+W_rot.shape[0], j:j+W_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * W_rot))\n",
    "    return(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:38.053870Z",
     "iopub.status.busy": "2025-01-14T02:50:38.053543Z",
     "iopub.status.idle": "2025-01-14T02:50:38.058702Z",
     "shell.execute_reply": "2025-01-14T02:50:38.057909Z",
     "shell.execute_reply.started": "2025-01-14T02:50:38.053832Z"
    },
    "id": "cPRr7Q-nsRzI"
   },
   "outputs": [],
   "source": [
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "W = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:38.059541Z",
     "iopub.status.busy": "2025-01-14T02:50:38.059346Z",
     "iopub.status.idle": "2025-01-14T02:50:38.067227Z",
     "shell.execute_reply": "2025-01-14T02:50:38.066379Z",
     "shell.execute_reply.started": "2025-01-14T02:50:38.059524Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737201151913,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "IAtcxuSisRzI",
    "outputId": "0b9ec84e-b7f4-4897-80ab-641bb75dbce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implementation:\n",
      " [[25. 24.]\n",
      " [28. 25.]]\n"
     ]
    }
   ],
   "source": [
    "print('Conv2d Implementation:\\n', conv2d(X, W, p=(0, 0), s=(1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:38.067892Z",
     "iopub.status.busy": "2025-01-14T02:50:38.067693Z",
     "iopub.status.idle": "2025-01-14T02:50:38.073612Z",
     "shell.execute_reply": "2025-01-14T02:50:38.072844Z",
     "shell.execute_reply.started": "2025-01-14T02:50:38.067874Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737201151913,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "3-8SYVQwsRzJ",
    "outputId": "46b58009-b130-447a-e181-4e3433ef8fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy Results:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "print('SciPy Results:\\n', scipy.signal.convolve2d(X, W, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEJ6rcEasRzJ"
   },
   "source": [
    "### Example 3. Convolution as an image filter\n",
    "\n",
    "Consider the image below, convolved the image with the following kernel and see the results:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{pmatrix}$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/25600601/134188807-93c3ede8-2694-407f-bd5d-eb4185df683d.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:38.074447Z",
     "iopub.status.busy": "2025-01-14T02:50:38.074256Z",
     "iopub.status.idle": "2025-01-14T02:50:39.098345Z",
     "shell.execute_reply": "2025-01-14T02:50:39.097421Z",
     "shell.execute_reply.started": "2025-01-14T02:50:38.074430Z"
    },
    "executionInfo": {
     "elapsed": 3614,
     "status": "ok",
     "timestamp": 1737201155526,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "BK0g068TsRzJ",
    "outputId": "6e523f3f-f6b9-4f6a-e697-2bb27b8c89ce"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io, color\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exposure\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from skimage import io, color\n",
    "from skimage import exposure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = io.imread(img_dir +'image.jpg') # Load the image\n",
    "img = color.rgb2gray(img) # Convert the image to grayscale (1 channel)\n",
    "\n",
    "#kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "#kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]]) #sharpen\n",
    "kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])  #edge detection\n",
    "#kernel = np.array([[1,1,1],[1,1,1],[1,1,1]])/9.0   #box blur\n",
    "#kernel = np.array([[0,0,0],[0,1,0],[0,0,0]])\n",
    "#kernel = np.array([[-2,4,-2],[-2,4,-2],[-2,4,-2]])\n",
    "\n",
    "# you can use 'valid' instead of 'same', then it will not add zero padding\n",
    "image_sharpen = scipy.signal.convolve2d(img, kernel, 'same')\n",
    "#print '\\n First 5 columns and rows of the image_sharpen matrix: \\n', image_sharpen[:5,:5]*255\n",
    "\n",
    "# Adjust the contrast of the filtered image by applying Histogram Equalization\n",
    "image_sharpen_equalized = exposure.equalize_adapthist(image_sharpen/np.max(np.abs(image_sharpen)), clip_limit=0.03)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.imshow(image_sharpen_equalized, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:39.099360Z",
     "iopub.status.busy": "2025-01-14T02:50:39.099059Z",
     "iopub.status.idle": "2025-01-14T02:50:39.480649Z",
     "shell.execute_reply": "2025-01-14T02:50:39.479777Z",
     "shell.execute_reply.started": "2025-01-14T02:50:39.099340Z"
    },
    "executionInfo": {
     "elapsed": 1458,
     "status": "ok",
     "timestamp": 1737201156982,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "o5lxddL660fH",
    "outputId": "a25b2d3c-466b-4f0f-d290-19edd75e85ff"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCGTE6Nq0Q8r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## ‚ö†Ô∏è Checkpoint ‚ö†Ô∏è\n",
    "In the next 5-10 minutes, play around with the above operations (kernels) and see how the filters can emphasize specific features. The weights of these filters are what a CNN learns!\n",
    "\n",
    "</div>\n",
    "\n",
    "Another example is shown below, try to duplicate.\n",
    "\n",
    "<img width=\"694\" alt=\"effect_filters\" src=\"https://user-images.githubusercontent.com/25600601/134189067-43992234-3dab-4df6-b935-13a3d7b40339.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8bT9eQgsRzK"
   },
   "source": [
    "There is nothing new with the idea of convolution, it has been used even before CNNs to isolate pictures of a given image and the list of the most common kernels can be seen here: https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "\n",
    "![Convolution_Image](https://user-images.githubusercontent.com/25600601/134189184-b6f76ae9-0445-4e26-bd72-e2200cd36dee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChZcwGkWsRzK"
   },
   "source": [
    "# 3. Subsampling or Pooling\n",
    "\n",
    "Subsampling is typically applied in two forms of pooling operations in convolutional neural networks: **max-pooling** and **mean-pooling** (also known as **average-pooling**). Here, max-pooling takes the maximum value from a neighborhood of pixels, and mean-pooling computes their average. The pooling layer is usually denoted by $\\mathbf{P}_{n1√ón2}$. Here, the subscript determines the size of the neighborhood (the number of adjacent pixels in each dimension), where the max or mean operation is performed. We refer to such a neighborhood as the pooling size.\n",
    "\n",
    "The advantage of pooling are two-fold:\n",
    "\n",
    "1. Pooling (max-pooling) introduces **some local invariance**. This means that small changes in a local neighborhood do not change the result of max-pooling. Therefore, it helps **generate features that are more robust to noise** in the input data. See the following example that shows max-pooling of two different input matrices $\\mathbf{X}_1$ and $\\mathbf{X}_2$ results in the same output.     \n",
    "\n",
    "<img width=\"911\" alt=\"max_pooling\" src=\"https://user-images.githubusercontent.com/25600601/134189420-0be5f32b-efc9-4125-911d-beaf2917491d.png\">\n",
    "\n",
    "2. Pooling **decreases the size of features**, which results in higher computational efficiency. Furthermore, reducing the number of features may reduce the degree of overfitting as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5evGqP-sRzK"
   },
   "source": [
    "**Note:** Traditionally, pooling is assumed to be nonoverlapping. Pooling is typically performed on nonoverlapping neighborhoods, which can be done by setting the stride parameter equal to the pooling size. For example, a nonoverlapping pooling layer $\\mathbf{P}_{n1√ón2}$ requires\n",
    "a stride parameter $s=(n_1, n_2)$.    \n",
    "\n",
    "On the other hand, overlapping pooling occurs if the stride is smaller than pooling size. An example where overlapping pooling is used in a convolutional network is described in ImageNet Classification with Deep Convolutional Neural Networks, A. Krizhevsky, I. Sutskever, and G. Hinton, 2012, which is freely available as a manuscript at https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutionalneural-networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px1qedlbsRzK"
   },
   "source": [
    "# 4. Putting everything together to build a CNN\n",
    "\n",
    "So far, we've learned about the basic building blocks of convolutional neural networks. The concepts illustrated in this chapter are not really more difficult than traditional multilayer neural networks. Intuitively, we can say that the most important operation in a traditional neural network is the matrix-vector multiplication. For instance, we use matrix-vector multiplications to pre-activations (or net input) as in $\\mathbf{a} = \\mathbf{W} \\mathbf{x} + b$. Here, $\\mathbf{x}$ is a column vector representing pixels, and $\\mathbf{W}$ is the weight matrix connecting the pixel inputs to each hidden unit.\n",
    "\n",
    "In a convolutional neural network, this operation is replaced by a convolution operation, as in $\\mathbf{A} = \\mathbf{W} \\mathbf{X} + b$, where $\\mathbf{X}$ is a matrix representing the pixels in a height $x$ width arrangement. In both cases, the pre-activations are passed to an activation function to obtain the activation of a hidden unit $\\mathbf{H}$ =$ \\Phi(\\mathbf{A})$, where $\\Phi$ is the activation function. Furthermore, recall that subsampling is another building block of a convolutional neural network, which may appear in the form of pooling, as we described in the previous section.\n",
    "\n",
    "<img width=\"844\" alt=\"CNN_basic_architecture\" src=\"https://user-images.githubusercontent.com/25600601/134189678-2790c595-39c7-463c-87f5-32992cfa03ca.png\">\n",
    "\n",
    "Shown below is another representation of how the convolutional layer is used\n",
    "\n",
    "<img width=\"672\" alt=\"CNN_layer1\" src=\"https://user-images.githubusercontent.com/25600601/134189776-6061b535-1f98-451d-b0e3-e1c75d580490.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j14DDRMlsRzL"
   },
   "source": [
    "Convolutions operate over 3D tensors, called **feature maps**, with two spatial axes (height and width) as well as a depth axis (also called the channels axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits in the next example, the depth is 1 (levels of gray). The convolution operation extracts patches from its input feature\n",
    "map and applies the same transformation to all of these patches, producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for filters. Filters encode specific aspects of the input data: at a\n",
    "high level, a single filter could encode the concept ‚Äúpresence of a face in the input,‚Äù for instance.\n",
    "\n",
    "<img width=\"557\" alt=\"cat_CNN\" src=\"https://user-images.githubusercontent.com/25600601/134189961-77c6ac3e-842b-46d9-a9ed-a576f6979eb9.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVe6GIP7sRzL"
   },
   "source": [
    "## Example 4. MNIST data set using a small CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv0Q3gKvsRzL"
   },
   "source": [
    "A CNN or convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension). In this case, we‚Äôll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We‚Äôll do this by passing the argument input_shape=(28, 28, 1) to the first layer. By default each feature map has a bias inherently added in keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6iSoVK0Q8s"
   },
   "source": [
    "### Step 1. Instantiating a small convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:39.481677Z",
     "iopub.status.busy": "2025-01-14T02:50:39.481459Z",
     "iopub.status.idle": "2025-01-14T02:50:41.043218Z",
     "shell.execute_reply": "2025-01-14T02:50:41.042347Z",
     "shell.execute_reply.started": "2025-01-14T02:50:39.481657Z"
    },
    "executionInfo": {
     "elapsed": 3359,
     "status": "ok",
     "timestamp": 1737201160339,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "-ZUx_GvGvBu3",
    "outputId": "9117a42b-8d27-424b-aa1d-c10d8427c75a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Net()\n",
    "\n",
    "# Optional: Print model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjTKa8tCsRzL"
   },
   "source": [
    "Let‚Äôs display the architecture of the convnet so far, explain the Parameter numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:41.044258Z",
     "iopub.status.busy": "2025-01-14T02:50:41.043965Z",
     "iopub.status.idle": "2025-01-14T02:50:41.048633Z",
     "shell.execute_reply": "2025-01-14T02:50:41.047713Z",
     "shell.execute_reply.started": "2025-01-14T02:50:41.044239Z"
    },
    "executionInfo": {
     "elapsed": 4047,
     "status": "ok",
     "timestamp": 1737201164384,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "COG-8WRcvBu3",
    "outputId": "d456685f-e322-4ede-d5ce-432336ade033"
   },
   "outputs": [],
   "source": [
    "!pip install -U torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:41.049569Z",
     "iopub.status.busy": "2025-01-14T02:50:41.049363Z",
     "iopub.status.idle": "2025-01-14T02:50:41.609398Z",
     "shell.execute_reply": "2025-01-14T02:50:41.608499Z",
     "shell.execute_reply.started": "2025-01-14T02:50:41.049551Z"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1737201164941,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "N0yECpM7vBu3",
    "outputId": "5c2595e9-831b-43cf-a281-a09973695fec"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = Net()  # Assuming Net is your model class\n",
    "summary(model, input_size=(1, 1, 28, 28), col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__ZvRgdpsRzM"
   },
   "source": [
    "You can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).\n",
    "\n",
    "Note that the default padding is zero $p=0$ and default stride is 1 $s=1$.\n",
    "\n",
    "The 320 parameters in the first layer is because each image is convolved to a kernel/filter (weight) equal to (3 $\\times$ 3) using 32 feature maps (3 $\\times$ 3 $\\times$ 32 = 9 $\\times$ 32 = 288). Add a bias node for each of kernel in the 32 feature maps (1 $\\times$ 32=32), and the total parameters is 288 + 32 = 320.\n",
    "\n",
    "Max pooling will not add any new parameter but passing it into another 3 $\\times$ 3 filter this time producing 64 feature maps for every 32 feature maps previously produce we have a total of  (32 $\\times$ 64  $\\times$ 3  $\\times$ 3 =18,432) parameters. Add to this the bias for each of the 64 feature maps and the total is (18,432 + 64) 18,496. The process continues for the suceeding layers.\n",
    "\n",
    "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you‚Äôre already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy0rGAFbvBu3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## üí° Keras vs. PyTorch\n",
    "If you compare the below code cell with the implementation in Keras, the Keras implementation is so much simpler. This is because Keras is a high-level framework (which uses Tensorflow under the hood), whereas PyTorch is more low level. The low level framework allows for more customization, but as a consequence would need more boilerplate code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ6dsDrH0Q8t"
   },
   "source": [
    "### Step 2. Adding a classifier on top of the convnet\n",
    "\n",
    "<img width=\"704\" alt=\"CNN_full\" src=\"https://user-images.githubusercontent.com/25600601/134190140-22451833-879f-4696-b365-7eb22dcd3933.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:41.610371Z",
     "iopub.status.busy": "2025-01-14T02:50:41.610148Z",
     "iopub.status.idle": "2025-01-14T02:50:41.620261Z",
     "shell.execute_reply": "2025-01-14T02:50:41.619421Z",
     "shell.execute_reply.started": "2025-01-14T02:50:41.610352Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737201164941,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "ALcwL3WSsRzM",
    "outputId": "42934f2f-46ab-474d-82e1-09a5d83e7936",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Added layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(576, 64)  # 64 channels, 7x7 spatial dimension after previous layers\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "\n",
    "        # Added layers\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = Net()\n",
    "\n",
    "# Optional: Print model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nakQKCkCsRzM"
   },
   "source": [
    "We‚Äôll do 10-way classification, using a final layer with 10 outputs and a softmax activation.\n",
    "Here‚Äôs what the network looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:41.621099Z",
     "iopub.status.busy": "2025-01-14T02:50:41.620903Z",
     "iopub.status.idle": "2025-01-14T02:50:41.641596Z",
     "shell.execute_reply": "2025-01-14T02:50:41.640813Z",
     "shell.execute_reply.started": "2025-01-14T02:50:41.621082Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737201164941,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "leVyYa3xvBu-",
    "outputId": "27b80271-bcbd-4d2d-9b4e-5b0488fa1e1a"
   },
   "outputs": [],
   "source": [
    "model = Net()  # Assuming Net is your model class\n",
    "summary(model, input_size=(1, 1, 28, 28), col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLCcwDqisRzM"
   },
   "source": [
    "As you can see above, the (3, 3, 64) outputs are flattened into vectors of shape (576,) before going through two Dense layers. The total parameters if we use a 64 hidden nodes on the flattened 576 features is (64 $\\times$ 576 = 36,864) plus a built in biased in each of the 64 hidden nodes we get 36,928. Another small layer is added that contributes (64 $\\times$ 10 + 10 = 650) parameters.\n",
    "\n",
    "Now, let‚Äôs train the convnet on the MNIST digits. We‚Äôll reuse a lot of the code from the MNIST example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4-LBvBGvBu-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## üí° Keras vs. PyTorch\n",
    "Again, if you compare the below cell with the implementation in Keras, the PyTorch version is more verbose. In this case, we actually define the entire training loop. To avoid clutter in practice, we would have this in a separate Python script which we can just import.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUjzrHZasRzM"
   },
   "source": [
    "### Step 3. Training the convnet on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:41.642534Z",
     "iopub.status.busy": "2025-01-14T02:50:41.642332Z",
     "iopub.status.idle": "2025-01-14T02:50:42.781862Z",
     "shell.execute_reply": "2025-01-14T02:50:42.781081Z",
     "shell.execute_reply.started": "2025-01-14T02:50:41.642516Z"
    },
    "executionInfo": {
     "elapsed": 2446,
     "status": "ok",
     "timestamp": 1737201167385,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "oGJPZL2YvBu-",
    "outputId": "a919ffd3-9878-4f06-e65f-ed083d3f8435"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Training on GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Training on CPU.\")\n",
    "\n",
    "# Move model to the device\n",
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:42.782780Z",
     "iopub.status.busy": "2025-01-14T02:50:42.782494Z",
     "iopub.status.idle": "2025-01-14T02:50:42.787863Z",
     "shell.execute_reply": "2025-01-14T02:50:42.786968Z",
     "shell.execute_reply.started": "2025-01-14T02:50:42.782760Z"
    },
    "id": "kmNtkDN6vBu-"
   },
   "outputs": [],
   "source": [
    "# Declare relevant parameters\n",
    "BATCH_SIZE = 150\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:50:42.788664Z",
     "iopub.status.busy": "2025-01-14T02:50:42.788472Z",
     "iopub.status.idle": "2025-01-14T02:51:13.402763Z",
     "shell.execute_reply": "2025-01-14T02:51:13.401072Z",
     "shell.execute_reply.started": "2025-01-14T02:50:42.788648Z"
    },
    "executionInfo": {
     "elapsed": 32881,
     "status": "ok",
     "timestamp": 1737201780495,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "baywWsynvBu-",
    "outputId": "1b6322b8-2575-4fa0-c289-9994998d06c8"
   },
   "outputs": [],
   "source": [
    "# # Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to tensor and scales to [0, 1]\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Prepare DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:13.404573Z",
     "iopub.status.busy": "2025-01-14T02:51:13.404250Z",
     "iopub.status.idle": "2025-01-14T02:51:13.577227Z",
     "shell.execute_reply": "2025-01-14T02:51:13.576599Z",
     "shell.execute_reply.started": "2025-01-14T02:51:13.404546Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1737201780498,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "HxJVdfbDsRzN",
    "outputId": "6fefd180-2590-4329-e5e0-3bf2449da385",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sample_number = 23\n",
    "first_image = data[sample_number].cpu()\n",
    "first_image = np.array(first_image, dtype='float')\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "print(\"train label =\", target[sample_number].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFzTLv6sRzO"
   },
   "source": [
    "### Step 4. Let‚Äôs evaluate the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:13.578308Z",
     "iopub.status.busy": "2025-01-14T02:51:13.578039Z",
     "iopub.status.idle": "2025-01-14T02:51:14.429394Z",
     "shell.execute_reply": "2025-01-14T02:51:14.428083Z",
     "shell.execute_reply.started": "2025-01-14T02:51:13.578281Z"
    },
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1737201782277,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "rvb9GebMvBu_",
    "outputId": "7df44af7-af21-4aad-8386-5620d7abec86"
   },
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad(): # No need to track gradients for evaluation\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)  # Move data to device\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_acc = correct / total\n",
    "\n",
    "print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {100. * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:14.432145Z",
     "iopub.status.busy": "2025-01-14T02:51:14.431935Z",
     "iopub.status.idle": "2025-01-14T02:51:14.525899Z",
     "shell.execute_reply": "2025-01-14T02:51:14.524893Z",
     "shell.execute_reply.started": "2025-01-14T02:51:14.432128Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737201782279,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "3k6EJCynsRzO",
    "outputId": "21132eb6-6c65-4fe5-b895-25af0bec1705"
   },
   "outputs": [],
   "source": [
    "sample_number = 24\n",
    "first_image = data[sample_number].cpu()\n",
    "first_image = np.array(first_image, dtype='float')\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "print(\"test label =\", target[sample_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:14.526658Z",
     "iopub.status.busy": "2025-01-14T02:51:14.526481Z",
     "iopub.status.idle": "2025-01-14T02:51:19.991737Z",
     "shell.execute_reply": "2025-01-14T02:51:19.990868Z",
     "shell.execute_reply.started": "2025-01-14T02:51:14.526643Z"
    },
    "executionInfo": {
     "elapsed": 4356,
     "status": "ok",
     "timestamp": 1737201786633,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "Gc9H6JZZsRzP",
    "outputId": "076001f3-9bfd-4e14-806d-81751a60bf63"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "img_path = img_dir + '8_test.png'\n",
    "\n",
    "img = image.load_img(img_path, target_size=(28, 28),color_mode=\"grayscale\")\n",
    "img_invert = PIL.ImageOps.invert(img)\n",
    "x = image.img_to_array(img_invert)\n",
    "img = x.reshape(1,1,28,28) / 255.\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.imshow(img[0,0,:,:],cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "with torch.no_grad():\n",
    "    Prediction=model.forward(torch.from_numpy(img).float().to(device))\n",
    "    print(\"Predicted number: \", np.argmax(Prediction.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:19.993129Z",
     "iopub.status.busy": "2025-01-14T02:51:19.992540Z",
     "iopub.status.idle": "2025-01-14T02:51:20.106680Z",
     "shell.execute_reply": "2025-01-14T02:51:20.105750Z",
     "shell.execute_reply.started": "2025-01-14T02:51:19.993109Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737201786634,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "IhwEGSN-sRzP",
    "outputId": "df745f9f-d851-4e10-9feb-fa3a8e7fd625",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYN2DiQH0Q8z"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## ‚ö†Ô∏è Checkpoint ‚ö†Ô∏è\n",
    "\n",
    "In the next 5-10 minutes, play around with the basic CNN and share interesting findings if any from your team's experiments!\n",
    "\n",
    "As an additional exercise, generate your own number image 0-9 and test the above system using model.predict($\\mathbf{x}$) where $\\mathbf{x}$ is the  28 $\\times$ 28 grayscale of the image that you have generated.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHdnabEhsRzQ"
   },
   "source": [
    "# 5. Reviewing Neural Network Basics\n",
    "\n",
    "## 5.1 What does \"sample\", \"batch\", and \"epoch\" mean?\n",
    "\n",
    "Below are some common definitions that are necessary to know and understand to correctly utilize Keras:\n",
    "\n",
    "**Sample**: one element of a dataset.   \n",
    "- **Example**: one image is a sample in a convolutional network   \n",
    "- **Example**: one audio file is a sample for a speech recognition model   \n",
    "\n",
    "**Batch**: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.   \n",
    "A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluating/prediction).\n",
    "\n",
    "**Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. When using evaluation_data or evaluation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SyyPA_qsRzQ"
   },
   "source": [
    "## 5.2 Final Error Activation Function\n",
    "\n",
    "Below is a general strategy on what Error function to use given the final activation function.\n",
    "\n",
    "<img width=\"744\" alt=\"cheat_sheet_error_function1\" src=\"https://user-images.githubusercontent.com/25600601/134190593-b64c107e-b0fc-45a7-83bd-050cac62f0d5.png\">\n",
    "\n",
    "### Activation functions\n",
    "\n",
    "Given below are the most common and most successful activation functions\n",
    "\n",
    "RELU - REctified Linear Unit function\n",
    "\n",
    "<img width=\"400\" alt=\"relu\" src=\"https://user-images.githubusercontent.com/25600601/134190706-6e9b2553-f9e5-4bcc-8adf-adbc5c1b2362.png\">\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "<img width=\"400\" alt=\"sigmoid\" src=\"https://user-images.githubusercontent.com/25600601/134190855-6324c76a-bd67-476d-9b44-cb3da89bf112.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQP9SBs5sRzQ"
   },
   "source": [
    "Softmax is normalized exponential given by:\n",
    "\n",
    "\\begin{equation}\n",
    "P(j,\\mathbf{x}) = \\frac{\\exp(\\mathbf{w}_j \\mathbf{x}^T)}{\\sum_{k=1}^K \\exp(\\mathbf{w}_k \\mathbf{x}^T)}\n",
    "\\end{equation}\n",
    "\n",
    "<img width=\"400\" alt=\"softmax\" src=\"https://user-images.githubusercontent.com/25600601/134190943-9b1b7ac1-8010-4acf-9e76-b9a4605521dc.png\">\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:20.107501Z",
     "iopub.status.busy": "2025-01-14T02:51:20.107316Z",
     "iopub.status.idle": "2025-01-14T02:51:20.113942Z",
     "shell.execute_reply": "2025-01-14T02:51:20.113057Z",
     "shell.execute_reply.started": "2025-01-14T02:51:20.107485Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737201786634,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "H09tL-tVsRzQ",
    "outputId": "863bb5b3-5e95-41f8-e7fe-da080da21c39"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "softmax = np.exp(z)/np.sum(np.exp(z))\n",
    "softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad_h1lAPsRzQ"
   },
   "source": [
    "\n",
    "### Error functions\n",
    "\n",
    "Below are the implementation of the error functions [$yHAT$ = output of NN ($\\Psi_{NN}$), y = theoretical output ($\\Psi_{th}$]:\n",
    "\n",
    "\n",
    "**MSE (Mean Square Error):** Usually used for regresssion problems\n",
    "\n",
    "$\\frac{1}{2}(\\hat{y} - y)^2$\n",
    "\n",
    "\n",
    "**Binary Cross Entropy:** Use for classification problems, Outputs are independent for each class or the loss computed for every output is not affected by other outputs.\n",
    "\n",
    "$‚àí(y \\log(\\hat{y})+(1‚àíy)\\log(1 ‚àí \\hat{y}))$\n",
    "\n",
    "**Categorical Cross Entropy:** Use for classification problems, outputs are dependent for each class or the loss computed for every output is affected by the computed outputs of other classes (most widely used activation is softmax as described in the above cell).\n",
    "\n",
    "$-\\sum_{c=1}^{M} y_{o,c} \\log (p_{o,c})$\n",
    "\n",
    "\n",
    "M - number of classes (dog, cat, fish, birds)           \n",
    "y - binary indicator (0 or 1) if class label c is the correct classification for observation o      \n",
    "p - predicted probability observation o is of class c   \n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "\n",
    "Given below are the implementations in numpy for the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T02:51:20.114582Z",
     "iopub.status.busy": "2025-01-14T02:51:20.114391Z",
     "iopub.status.idle": "2025-01-14T02:51:20.119798Z",
     "shell.execute_reply": "2025-01-14T02:51:20.118889Z",
     "shell.execute_reply.started": "2025-01-14T02:51:20.114565Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737201786635,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "G-GGozJBsRzQ"
   },
   "outputs": [],
   "source": [
    "def MSE(yHat, y):\n",
    "    return np.sum((yHat - y)**2) / y.size\n",
    "\n",
    "#Binary Cross Entropy\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "      return -log(yHat)\n",
    "    else:\n",
    "      return -log(1 - yHat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqzK_JhMsRzQ"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "The rmsprop optimizer is generally a good enough choice, whatever your problem. That‚Äôs one less thing for you to worry about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRa2Syjr0Q80"
   },
   "source": [
    "# Additional resources in re-implementing the above:\n",
    "\n",
    "- https://towardsdatascience.com/visualizing-intermediate-activations-of-a-cnn-trained-on-the-mnist-dataset-2c34426416c8\n",
    "- https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0\n",
    "    1. Note how the middle layers are being formed in both links above. Read the conclusions at the end of their discussion. Are these intuitive to you now?\n",
    "    2. Check how the best parameters are saved using \"checkpointer\" in the second link.\n",
    "- Here is a list of available data for potential individual/group projects: https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research\n",
    "- CNN Explainer: https://poloclub.github.io/cnn-explainer/\n",
    "- ImageNet State of the Art per Year: https://paperswithcode.com/sota/image-classification-on-imagenet\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "13BWFt7BEBHYqewnyvO8pADrZT7lIZRLV",
     "timestamp": 1715836041149
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "msds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
