{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUEoAw3KZ9nQ"
   },
   "source": [
    "# (PyTorch) Notebook 1B. Data Augmentation for Convolutional Neural Networks (CNN)\n",
    "---\n",
    "Organized and prepared by Christopher Monterola, updated by Kenneth Co.\n",
    "\n",
    "This notebook was conceptualized, organized, and primarily prepared for the **Machine Learning** courses.\n",
    "\n",
    "### This notebook uses the following references:\n",
    "1. Python Machine Learning, Second Edition, Sebastian Raschka and Vahid Mirjalili, Packt Publishing Ltd. Birmingham B3 2PB, UK Sept 2017.\n",
    "2. Hands-On Machine Learning with Scikit-Learn and TensorFlow, Aurélien Géron, O'Reilly 2017.\n",
    "3. Deep Learning with Python, Francois Chollet, Manning New York 2018.\n",
    "4. 2018 Google: https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part1.ipynb\n",
    "\n",
    "Here we illustrate how data augmentation using Keras for CNN can improve accuracy (avoid overfitting). We use the 2013 Cats vs. Dogs dataset to illustrate the methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0Ocn2x8oLbQ"
   },
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:36.339560Z",
     "iopub.status.busy": "2025-01-14T07:00:36.339321Z",
     "iopub.status.idle": "2025-01-14T07:00:36.344416Z",
     "shell.execute_reply": "2025-01-14T07:00:36.343451Z",
     "shell.execute_reply.started": "2025-01-14T07:00:36.339527Z"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1737116345402,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "2ckAPndjoLbQ"
   },
   "outputs": [],
   "source": [
    "# !pip install -U keras\n",
    "# !pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:36.345324Z",
     "iopub.status.busy": "2025-01-14T07:00:36.345106Z",
     "iopub.status.idle": "2025-01-14T07:00:36.353543Z",
     "shell.execute_reply": "2025-01-14T07:00:36.351293Z",
     "shell.execute_reply.started": "2025-01-14T07:00:36.345302Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737116345731,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "cQ6N89RyoLbS"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = '/content/drive/MyDrive/COSCI224 Machine Learning 3 Notebooks/data/'\n",
    "# IMG_DIR = '/content/drive/MyDrive/COSCI224 Machine Learning 3 Notebooks/images/'\n",
    "# MODEL_DIR = '/content/drive/MyDrive/COSCI224 Machine Learning 3 Notebooks/models/'\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "IMG_DIR = 'images/'\n",
    "MODEL_DIR = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd3ErYdBZ9nU"
   },
   "source": [
    "# The Dogs vs. Cats Dataset (Kaggle 2013)\n",
    "\n",
    "The Dogs vs. Cats dataset that you’ll use isn’t packaged with Keras. It was made available by Kaggle as part of a computer-vision competition in late 2013, back when CNN weren’t mainstream. You can download the original dataset from [here](https://www.kaggle.com/c/dogs-vs-cats/data).\n",
    "\n",
    "The pictures are medium-resolution color JPEGs, and here are some examples:\n",
    "\n",
    "![cats_vs_dogs_samples](https://user-images.githubusercontent.com/25600601/134775470-7cf33e7e-f2d0-4a89-85b1-4963bf1c99da.jpg)\n",
    "\n",
    "Unsurprisingly, the dogs-vs-cats Kaggle competition in 2013 was won by entrants who used CNN. The best entries achieved up to 95% accuracy. In this example, you’ll get up to $83\\%$ accuracy for test set even though you’ll train your models on less than 10% of the data that was available to the competitors.\n",
    "\n",
    "This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed). After downloading and uncompressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wryTE14eZ9nU"
   },
   "source": [
    "# Step 1. Get the data\n",
    "- Copy images to training, validation, and test directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WAna6pUWoLbS"
   },
   "source": [
    "Use these commands if you want download and run on your own directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:36.355475Z",
     "iopub.status.busy": "2025-01-14T07:00:36.354989Z",
     "iopub.status.idle": "2025-01-14T07:00:36.359984Z",
     "shell.execute_reply": "2025-01-14T07:00:36.359064Z",
     "shell.execute_reply.started": "2025-01-14T07:00:36.355454Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737116345731,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "HAzcerZwcwVd"
   },
   "outputs": [],
   "source": [
    "##import os, shutil\n",
    "\n",
    "#The path to the directory where the original\n",
    "## dataset was uncompressed\n",
    "#original_dataset_dir = DATA_DIR +'dogs_vs_cats/train'\n",
    "\n",
    "#The directory where we will\n",
    "## store our smaller dataset\n",
    "#base_dir = 'dogs_vs_cats/results/'\n",
    "#os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:36.360717Z",
     "iopub.status.busy": "2025-01-14T07:00:36.360523Z",
     "iopub.status.idle": "2025-01-14T07:00:40.558767Z",
     "shell.execute_reply": "2025-01-14T07:00:40.556980Z",
     "shell.execute_reply.started": "2025-01-14T07:00:36.360700Z"
    },
    "executionInfo": {
     "elapsed": 1060,
     "status": "ok",
     "timestamp": 1737116346789,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "-O3l2rq_oLbT",
    "outputId": "fa1a33f6-c370-429c-9a66-8912546e705b"
   },
   "outputs": [],
   "source": [
    "# # Use this to download the data\n",
    "# !wget --no-check-certificate \\\n",
    "#     https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
    "#     -O /tmp/cats_and_dogs_filtered.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:40.561468Z",
     "iopub.status.busy": "2025-01-14T07:00:40.560817Z",
     "iopub.status.idle": "2025-01-14T07:00:41.388545Z",
     "shell.execute_reply": "2025-01-14T07:00:41.386727Z",
     "shell.execute_reply.started": "2025-01-14T07:00:40.561399Z"
    },
    "executionInfo": {
     "elapsed": 2221,
     "status": "ok",
     "timestamp": 1737116349008,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "56nsBeMJjpNn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "local_zip = 'cats_and_dogs_filtered.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.391518Z",
     "iopub.status.busy": "2025-01-14T07:00:41.391277Z",
     "iopub.status.idle": "2025-01-14T07:00:41.397178Z",
     "shell.execute_reply": "2025-01-14T07:00:41.396330Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.391498Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "DTDAVtfkjuXL"
   },
   "outputs": [],
   "source": [
    "base_dir = '/tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONnaFUj4j0HF"
   },
   "source": [
    "Here are now the filenames of the `cats` and `dogs` `train` directories (file naming conventions are the same in the `validation` directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.398095Z",
     "iopub.status.busy": "2025-01-14T07:00:41.397881Z",
     "iopub.status.idle": "2025-01-14T07:00:41.405070Z",
     "shell.execute_reply": "2025-01-14T07:00:41.404251Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.398078Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "k0i2J8iSjzaC",
    "outputId": "c43111f4-736e-42a0-83d0-e67d19c32de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[:10])\n",
    "\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.405878Z",
     "iopub.status.busy": "2025-01-14T07:00:41.405681Z",
     "iopub.status.idle": "2025-01-14T07:00:41.411545Z",
     "shell.execute_reply": "2025-01-14T07:00:41.410649Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.405861Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "WC2IlmtBZ9nZ",
    "outputId": "a90eeb17-26f0-4f30-d2dc-4f7fd50163b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.412207Z",
     "iopub.status.busy": "2025-01-14T07:00:41.412022Z",
     "iopub.status.idle": "2025-01-14T07:00:41.417748Z",
     "shell.execute_reply": "2025-01-14T07:00:41.416863Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.412191Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "jM4yWn4tZ9na",
    "outputId": "ef2ab8e0-a795-4052-a96c-8389425b3a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.418582Z",
     "iopub.status.busy": "2025-01-14T07:00:41.418393Z",
     "iopub.status.idle": "2025-01-14T07:00:41.423622Z",
     "shell.execute_reply": "2025-01-14T07:00:41.422859Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.418565Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "3YT0xOgzZ9nb",
    "outputId": "cddfbdd2-ca27-4ef7-ddbe-82022a0726d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.424242Z",
     "iopub.status.busy": "2025-01-14T07:00:41.424061Z",
     "iopub.status.idle": "2025-01-14T07:00:41.429698Z",
     "shell.execute_reply": "2025-01-14T07:00:41.428663Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.424226Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "47i2GnHFZ9nb",
    "outputId": "c5197c11-b84f-4295-b5ca-dc8816fcfec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTYDLidckkGq"
   },
   "source": [
    "Let's look at the Dogs and Cat Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.430548Z",
     "iopub.status.busy": "2025-01-14T07:00:41.430362Z",
     "iopub.status.idle": "2025-01-14T07:00:41.906492Z",
     "shell.execute_reply": "2025-01-14T07:00:41.905507Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.430531Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737116349009,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "u0CRoPAskaba"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
    "nrows = 4\n",
    "ncols = 4\n",
    "\n",
    "# Index for iterating over images\n",
    "pic_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:41.907452Z",
     "iopub.status.busy": "2025-01-14T07:00:41.907192Z",
     "iopub.status.idle": "2025-01-14T07:00:42.861617Z",
     "shell.execute_reply": "2025-01-14T07:00:42.860239Z",
     "shell.execute_reply.started": "2025-01-14T07:00:41.907433Z"
    },
    "executionInfo": {
     "elapsed": 6785,
     "status": "ok",
     "timestamp": 1737116355791,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "MPSSBOJmkf_V",
    "outputId": "208f7b27-34c6-485f-ef5f-9b189a6625ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(ncols * 4, nrows * 4)\n",
    "\n",
    "pic_index += 8\n",
    "next_cat_pix = [os.path.join(train_cats_dir, fname)\n",
    "                for fname in train_cat_fnames[pic_index-8:pic_index]]\n",
    "next_dog_pix = [os.path.join(train_dogs_dir, fname)\n",
    "                for fname in train_dog_fnames[pic_index-8:pic_index]]\n",
    "\n",
    "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
    "  # Set up subplot; subplot indices start at 1\n",
    "  sp = plt.subplot(nrows, ncols, i + 1)\n",
    "  sp.axis('Off') # Don't show axes (or gridlines)\n",
    "\n",
    "  img = mpimg.imread(img_path)\n",
    "  plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqWNkB06Z9nr"
   },
   "source": [
    "So you do indeed have 2,000 training images, 1,000 validation images, and 1,000 test images. Each split contains the same number of samples from each class: this is a balanced binary-classification problem, which means classification accuracy will be an appropriate measure of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzde3sqZ9nr"
   },
   "source": [
    "# Step 2. Build your NN\n",
    "In the previous example, we built a small CNN for MNIST. You’ll reuse the same general structure: the CNN will be a stack of alternated Conv2D (with relu activation) and MaxPooling2D layers. But because you’re dealing with bigger images and a more complex problem (previously we used MNIST with $28 \\times 28$ size and single channel), you’ll have to make your network larger.\n",
    "\n",
    "Accordingly, it will have one more Conv2D + MaxPooling2D stage. This serves both to augment the capacity of the network and to further reduce the size of the feature maps so they aren’t overly large when you reach the Flatten layer. Here, because you start from inputs of size 150 × 150 (a somewhat arbitrary choice), you end up with feature maps of size 7 × 7 just before the Flatten layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVx4xor9Z9nr"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Note:** The depth of the feature maps progressively increases in the network (from 32 to 128), whereas the size of the feature maps decreases (from 148 × 148 to 7 × 7).\n",
    "\n",
    "This is a pattern you’ll see in almost all CNN. Because you’re attacking a binary-classification problem, you’ll end the network with a single unit (a dense layer of size 1) and a threshold function (we will be using sigmoid activation). This unit will encode the probability that the network is looking at one class or the other.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LNVLMYDZ9ns"
   },
   "source": [
    "## Instantiate a small CNN for Dogs vs. Cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:42.862650Z",
     "iopub.status.busy": "2025-01-14T07:00:42.862438Z",
     "iopub.status.idle": "2025-01-14T07:00:44.454292Z",
     "shell.execute_reply": "2025-01-14T07:00:44.452704Z",
     "shell.execute_reply.started": "2025-01-14T07:00:42.862631Z"
    },
    "executionInfo": {
     "elapsed": 5504,
     "status": "ok",
     "timestamp": 1737116361293,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "HTLGmX7mZ9ns"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=0) # Added padding to match Keras' default behavior\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=0)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding=0)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(6272, 512) # Calculated the input size to the first dense layer\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_Yu03sRZ9ns"
   },
   "source": [
    "Let’s look at how the dimensions of the feature maps change with every successive layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torch --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4443,
     "status": "ok",
     "timestamp": 1737116489279,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "9nM4S05kNdSh",
    "outputId": "c857a112-cc97-4853-bb8d-5cbc3d27bfcd"
   },
   "outputs": [],
   "source": [
    "# !pip install -U torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:44.455374Z",
     "iopub.status.busy": "2025-01-14T07:00:44.455083Z",
     "iopub.status.idle": "2025-01-14T07:00:45.060110Z",
     "shell.execute_reply": "2025-01-14T07:00:45.059262Z",
     "shell.execute_reply.started": "2025-01-14T07:00:44.455355Z"
    },
    "executionInfo": {
     "elapsed": 1405,
     "status": "ok",
     "timestamp": 1737116490682,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "97sDPdpQZ9ns",
    "outputId": "892fa8c3-d265-4408-975d-d85508594725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = Net()\n",
    "summary(model, input_size=(1, 3, 150, 150), col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NQw8oF3Z9ns"
   },
   "source": [
    "For the compilation step, you’ll go with the RMSprop optimizer, as usual. Because you ended the network with a single sigmoid unit, you’ll use binary crossentropy as the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKacK-F6Z9nt"
   },
   "source": [
    "# Step 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rexUSPAIZ9nt"
   },
   "source": [
    "As you know by now, data should be formatted into appropriately preprocessed floating point tensors before being fed into the network. Currently, the data sits on a drive as JPEG files, so the steps for getting it into the network are roughly as follows:\n",
    "\n",
    "1. Read the picture files.   \n",
    "2. Decode the JPEG content to RGB grids of pixels.   \n",
    "3. Convert these into floating-point tensors.   \n",
    "4. Rescale the pixel values (between 0 and 255) to the [0, 1] interval (neural networks prefer to deal with small input values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# %pip install torch==2.5.0 torchvision==0.20.0 --force-reinstall\n",
    "%pip install torch==2.0.0 torchvision==0.15.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:45.061127Z",
     "iopub.status.busy": "2025-01-14T07:00:45.060904Z",
     "iopub.status.idle": "2025-01-14T07:00:46.133898Z",
     "shell.execute_reply": "2025-01-14T07:00:46.133009Z",
     "shell.execute_reply.started": "2025-01-14T07:00:45.061109Z"
    },
    "executionInfo": {
     "elapsed": 10537,
     "status": "ok",
     "timestamp": 1737116501217,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "Ey-P9EZ5rfdC",
    "outputId": "72687ed9-5e81-4a45-c7d0-86bb868460dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "# Define the transformations for the training data\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Resize to the target size\n",
    "    transforms.ToTensor(),  # Convert to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Define the transformations for the validation data\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the training & validation dataset\n",
    "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
    "validation_dataset = ImageFolder(root=validation_dir, transform=validation_transform)\n",
    "\n",
    "# Create the training & validation dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Optional: Verify the class mapping (important for consistency with Keras)\n",
    "print(\"Class mapping:\", train_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffQ9bAEbZ9nu"
   },
   "source": [
    "Let’s look at the output of one of these generators: it yields batches of 150 × 150 RGB images (shape (25, 150, 150, 3)) and binary labels (shape (25,)). There are 25 samples in each batch (the batch size). Note that the generator yields these batches indefinitely: it loops endlessly over the images in the target folder.\n",
    "\n",
    "For this reason, you need to break the iteration loop at some point:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:46.134715Z",
     "iopub.status.busy": "2025-01-14T07:00:46.134429Z",
     "iopub.status.idle": "2025-01-14T07:00:46.466781Z",
     "shell.execute_reply": "2025-01-14T07:00:46.465200Z",
     "shell.execute_reply.started": "2025-01-14T07:00:46.134696Z"
    },
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1737116502026,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "y6_ZiNv2Z9nu",
    "outputId": "a8e9baca-83fa-461b-a20c-1e743b3eab8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Example of how to iterate through the data loaders\n",
    "for images, labels in train_loader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Labels batch shape:\", labels.shape)\n",
    "    break  # Just to show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SamlJNqOZ9nu"
   },
   "source": [
    "Let’s fit the model to the data using the generator. It expects as its first argument a Python generator that will yield batches of inputs and targets indefinitely, like this one does. Because the data is being generated, the Keras model needs to know how many samples to draw from the generator before declaring an epoch to be over.\n",
    "\n",
    "This is the role of the `steps_per_epoch` argument: after having drawn `steps_per_epoch` batches from the generator—that is, after having run for `steps_per_epoch` gradient descent steps—the fitting process will go to the next epoch. In this case, batches are 25 samples, so it will take 80 batches until you see your target of 2,000 samples.    \n",
    "\n",
    "When using `fit`, you can pass a `validation_data` argument, much as with the fit method. It’s important to note that this argument is allowed to be a data generator, but it could also be a tuple of Numpy arrays. If you pass a generator as `validation_data`, then this generator is expected to yield batches of `validation data` endlessly; thus you should also specify the `validation_steps` argument, which tells the process how many batches to draw from the validation generator for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pyokr3ZZ9nv"
   },
   "source": [
    "# Step 4. Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15WGpi48rfdD"
   },
   "source": [
    "Set up parameters and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:46.469781Z",
     "iopub.status.busy": "2025-01-14T07:00:46.469149Z",
     "iopub.status.idle": "2025-01-14T07:00:46.477339Z",
     "shell.execute_reply": "2025-01-14T07:00:46.475890Z",
     "shell.execute_reply.started": "2025-01-14T07:00:46.469714Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737116502026,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "ZYrPsJ1_rfdD"
   },
   "outputs": [],
   "source": [
    "# Declare relevant parameters\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:00:46.480287Z",
     "iopub.status.busy": "2025-01-14T07:00:46.479565Z",
     "iopub.status.idle": "2025-01-14T07:03:01.249435Z",
     "shell.execute_reply": "2025-01-14T07:03:01.247869Z",
     "shell.execute_reply.started": "2025-01-14T07:00:46.480233Z"
    },
    "executionInfo": {
     "elapsed": 419596,
     "status": "ok",
     "timestamp": 1737116921620,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "SqOM_XgLZ9nv",
    "outputId": "ea69bced-9205-4757-8d20-be1dd73e68a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Training on GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Training on CPU.\")\n",
    "\n",
    "# Move model to device\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Store training history\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1)) # Calculate loss - ensure labels are float and have correct shape\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        train_loss += loss.item() * images.size(0)  # Multiply by batch size to get total loss for this batch\n",
    "        preds = (outputs > 0.5).float() # Convert probabilities to binary predictions\n",
    "        train_correct += (preds == labels.float().unsqueeze(1)).sum().item() # Count correct predictions\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for images, labels in tqdm(validation_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "\n",
    "            # Statistics\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            val_correct += (preds == labels.float().unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Calculate average losses and accuracies for the epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    val_loss = val_loss / len(validation_loader.dataset)\n",
    "    val_acc = val_correct / len(validation_loader.dataset)\n",
    "\n",
    "    # Store the results in the history dictionary\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCs41_4FZ9nv"
   },
   "source": [
    "# Step 5. Saving the model\n",
    "It’s good practice to always save your models after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIRwDRbSrfdJ"
   },
   "source": [
    "## Step 5A Saving the whole model\n",
    "One option is to save the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "execution": {
     "iopub.execute_input": "2025-01-14T07:03:01.252448Z",
     "iopub.status.busy": "2025-01-14T07:03:01.251775Z",
     "iopub.status.idle": "2025-01-14T07:03:01.332685Z",
     "shell.execute_reply": "2025-01-14T07:03:01.331107Z",
     "shell.execute_reply.started": "2025-01-14T07:03:01.252381Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "error",
     "timestamp": 1737116921620,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "BybtQvvkZ9nv",
    "outputId": "41ecae22-d243-40c7-e326-2b035f95dbc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "torch.save(model, MODEL_DIR + '1B-cats_and_dogs_small.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd7ka3RtrfdK"
   },
   "source": [
    "To load the model, you first have to initialize the same model class (`Net` in this case), then load the weights from the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:03:01.336166Z",
     "iopub.status.busy": "2025-01-14T07:03:01.335945Z",
     "iopub.status.idle": "2025-01-14T07:03:01.356375Z",
     "shell.execute_reply": "2025-01-14T07:03:01.355531Z",
     "shell.execute_reply.started": "2025-01-14T07:03:01.336147Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1737116921620,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "BAvh4VEWrfdK"
   },
   "outputs": [],
   "source": [
    "loaded_model = torch.load(MODEL_DIR + '1B-cats_and_dogs_small.pth')\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUGmSEq_rfdK"
   },
   "source": [
    "## Step 5B Saving the trained model's state dictionary\n",
    "Another option is to only save and load the state dictionary. This method will require to have a properly defined model class that is compatible with the saved `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:03:01.357233Z",
     "iopub.status.busy": "2025-01-14T07:03:01.357023Z",
     "iopub.status.idle": "2025-01-14T07:03:01.408675Z",
     "shell.execute_reply": "2025-01-14T07:03:01.407119Z",
     "shell.execute_reply.started": "2025-01-14T07:03:01.357214Z"
    },
    "executionInfo": {
     "elapsed": 437240,
     "status": "aborted",
     "timestamp": 1737116921620,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "Fexb60HsrfdK"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_DIR + '1B-cats_and_dogs_small_state-dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:03:01.409661Z",
     "iopub.status.busy": "2025-01-14T07:03:01.409449Z",
     "iopub.status.idle": "2025-01-14T07:03:01.464450Z",
     "shell.execute_reply": "2025-01-14T07:03:01.463482Z",
     "shell.execute_reply.started": "2025-01-14T07:03:01.409643Z"
    },
    "executionInfo": {
     "elapsed": 437240,
     "status": "aborted",
     "timestamp": 1737116921620,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "4dMXnynKrfdK"
   },
   "outputs": [],
   "source": [
    "loaded_model = Net()  # Assuming 'Net' is your model class\n",
    "loaded_model.load_state_dict(torch.load(MODEL_DIR + '1B-cats_and_dogs_small_state-dict.pth', weights_only=True))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VPJLpmEoLbb"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## ❓ Question ❓\n",
    "What are the advantages and disadvantages of using `state_dict` versus saving and loading the entire model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTPVXW43Z9nw"
   },
   "source": [
    "Let’s plot the loss and accuracy of the model over the training and validation data during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "619Zwz1OZ9nw"
   },
   "source": [
    "# Step 6: Displaying curves of loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:03:01.465364Z",
     "iopub.status.busy": "2025-01-14T07:03:01.465135Z",
     "iopub.status.idle": "2025-01-14T07:03:01.726767Z",
     "shell.execute_reply": "2025-01-14T07:03:01.725926Z",
     "shell.execute_reply.started": "2025-01-14T07:03:01.465344Z"
    },
    "executionInfo": {
     "elapsed": 437240,
     "status": "aborted",
     "timestamp": 1737116921621,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "GEeRDnHaZ9nw"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history['train_acc']\n",
    "val_acc = history['val_acc']\n",
    "loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M9Zf_kQZ9nw"
   },
   "source": [
    "These plots are characteristic of **overfitting**. The training accuracy increases linearly over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 70–73%. The validation loss reaches its minimum after only five epochs and then stalls, whereas the training loss keeps decreasing linearly until it reaches nearly 0. Because you have relatively few training samples (2,000), overfitting will be your number-one concern.\n",
    "\n",
    "You already know about a number of techniques that can help mitigate overfitting such as L2 regularization (or weight decay). Another way is dropout or forced pruning of weights. We’re now going to work below with a new one, specific to computer vision and used almost universally when processing images with deep-learning models: data augmentation. This is very important when you have limited data for images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISLSMclRrfdL"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## ⚠️ Checkpoint ⚠️\n",
    "\n",
    "In the next 5-10 minutes, check with your LT that you understand everything that has happened so far.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfhh3HSdZ9nw"
   },
   "source": [
    "# Step 7: Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_yJ4Ix4Z9nw"
   },
   "source": [
    "Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data. Given infinite data, your model would be exposed to every possible aspect of the data distribution at hand: you would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better. In Keras, this can be done by configuring a number of random transformations to be performed on the images read by the ImageDataGenerator instance. Let’s get started with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH3Jz2zqZ9nx"
   },
   "source": [
    "Note we only do augmentation for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:06:21.340285Z",
     "iopub.status.busy": "2025-01-14T07:06:21.339570Z",
     "iopub.status.idle": "2025-01-14T07:06:21.350932Z",
     "shell.execute_reply": "2025-01-14T07:06:21.349143Z",
     "shell.execute_reply.started": "2025-01-14T07:06:21.340210Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "RfbqtsBhrfdM"
   },
   "outputs": [],
   "source": [
    "data_augment = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:06:21.354394Z",
     "iopub.status.busy": "2025-01-14T07:06:21.353756Z",
     "iopub.status.idle": "2025-01-14T07:06:21.361015Z",
     "shell.execute_reply": "2025-01-14T07:06:21.359439Z",
     "shell.execute_reply.started": "2025-01-14T07:06:21.354340Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "5vtaFY5erfdM"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:06:21.363255Z",
     "iopub.status.busy": "2025-01-14T07:06:21.362658Z",
     "iopub.status.idle": "2025-01-14T07:06:21.381562Z",
     "shell.execute_reply": "2025-01-14T07:06:21.380170Z",
     "shell.execute_reply.started": "2025-01-14T07:06:21.363203Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "eATl0YG7Z9nx"
   },
   "outputs": [],
   "source": [
    "# Create the training dataset with augmentation\n",
    "train_dataset_augment = ImageFolder(root=train_dir, transform=data_augment)\n",
    "train_loader_augment = DataLoader(train_dataset_augment, batch_size=BATCH_SIZE, shuffle=True, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muRP5VHIZ9ny"
   },
   "source": [
    "# Step 8: Adding Dropout\n",
    "To further combat overfitting, let's also add a **Dropout** layer to your model, right before the densely connected classifier.\n",
    "\n",
    "Dropout prevents overfitting by preventing a layer's \"over-reliance\" to some of the inputs that are normally biased by order of appearance or duplicity. Dropout was introduced by Srivastava and Hinton *et al*, and you can download the research paper here: http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\n",
    "\n",
    "<img width=\"981\" alt=\"dropout\" src=\"https://user-images.githubusercontent.com/25600601/134776375-ad90c14a-1a42-4ce9-8d84-13f4e6a99e26.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:06:21.783113Z",
     "iopub.status.busy": "2025-01-14T07:06:21.781976Z",
     "iopub.status.idle": "2025-01-14T07:06:21.803587Z",
     "shell.execute_reply": "2025-01-14T07:06:21.802067Z",
     "shell.execute_reply.started": "2025-01-14T07:06:21.783056Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "mJFzHNH6rfdM"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3), padding='valid') # Added padding to match Keras' default behavior\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding='valid')\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding='valid')\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 3), padding='valid')\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(6272, 512) # Calculated the input size to the first dense layer\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "        x = self.pool4(self.relu4(self.conv4(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:06:22.231803Z",
     "iopub.status.busy": "2025-01-14T07:06:22.231105Z",
     "iopub.status.idle": "2025-01-14T07:06:22.315608Z",
     "shell.execute_reply": "2025-01-14T07:06:22.314656Z",
     "shell.execute_reply.started": "2025-01-14T07:06:22.231748Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "d4yE1pz1rfdM"
   },
   "outputs": [],
   "source": [
    "model = Net()  # Assuming Net is your model class\n",
    "summary(model, input_size=(1, 3, 150, 150), col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFBkJs9_Z9ny"
   },
   "source": [
    "Let’s train the network using data augmentation and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:09:29.691008Z",
     "iopub.status.busy": "2025-01-14T07:09:29.690247Z",
     "iopub.status.idle": "2025-01-14T07:09:29.698636Z",
     "shell.execute_reply": "2025-01-14T07:09:29.696829Z",
     "shell.execute_reply.started": "2025-01-14T07:09:29.690948Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "hOKRMotfrfdM"
   },
   "outputs": [],
   "source": [
    "# Declare relevant parameters\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:09:30.051159Z",
     "iopub.status.busy": "2025-01-14T07:09:30.050248Z",
     "iopub.status.idle": "2025-01-14T07:14:33.896253Z",
     "shell.execute_reply": "2025-01-14T07:14:33.893354Z",
     "shell.execute_reply.started": "2025-01-14T07:09:30.051102Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "UkKK35n8Z9ny",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Training on GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Training on CPU.\")\n",
    "\n",
    "# Move model to device\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Store training history\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for images, labels in tqdm(train_loader_augment, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1)) # Calculate loss - ensure labels are float and have correct shape\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        train_loss += loss.item() * images.size(0)  # Multiply by batch size to get total loss for this batch\n",
    "        preds = (outputs > 0.5).float() # Convert probabilities to binary predictions\n",
    "        train_correct += (preds == labels.float().unsqueeze(1)).sum().item() # Count correct predictions\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for images, labels in tqdm(validation_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "\n",
    "            # Statistics\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            val_correct += (preds == labels.float().unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Calculate average losses and accuracies for the epoch\n",
    "    train_loss = train_loss / len(train_loader_augment.dataset)\n",
    "    train_acc = train_correct / len(train_loader_augment.dataset)\n",
    "    val_loss = val_loss / len(validation_loader.dataset)\n",
    "    val_acc = val_correct / len(validation_loader.dataset)\n",
    "\n",
    "    # Store the results in the history dictionary\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8r11ytRoLbd"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## ⚠️ Checkpoint ⚠️\n",
    "\n",
    "In the next 5-10 minutes, check with your LT that you understand everything that has happened so far. In particular:\n",
    "- How does **Data Augmentation** work and how did it help with overfitting?\n",
    "- How does **Dropout** work and how did it help with overfitting?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UHJp3E_Z9nz"
   },
   "source": [
    "# Step 9: Saving the model one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:14:38.437172Z",
     "iopub.status.busy": "2025-01-14T07:14:38.436366Z",
     "iopub.status.idle": "2025-01-14T07:14:38.513462Z",
     "shell.execute_reply": "2025-01-14T07:14:38.512800Z",
     "shell.execute_reply.started": "2025-01-14T07:14:38.437100Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "b3tjZ7tgZ9nz"
   },
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_DIR + '1B-cats_and_dogs_small_v2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_ZlZ7wtZ9nz"
   },
   "source": [
    "Let’s plot the results again. Thanks to data augmentation and dropout, you’re no longer overfitting: the training curves are closely tracking the validation curves. You now reach an accuracy of 83+\\%, a 11-12\\% relative improvement over the non-regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:14:39.757692Z",
     "iopub.status.busy": "2025-01-14T07:14:39.755921Z",
     "iopub.status.idle": "2025-01-14T07:14:40.042558Z",
     "shell.execute_reply": "2025-01-14T07:14:40.041224Z",
     "shell.execute_reply.started": "2025-01-14T07:14:39.757630Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "eqtZ5a4nZ9nz"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "acc = history['train_acc']\n",
    "val_acc = history['val_acc']\n",
    "loss = history['train_loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y5gYzm0Z9n0"
   },
   "source": [
    "# Step 10: Prototype testing\n",
    "Let's test below the performance of the system with images from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:09:01.109622Z",
     "iopub.status.busy": "2025-01-14T07:09:01.108916Z",
     "iopub.status.idle": "2025-01-14T07:09:01.225376Z",
     "shell.execute_reply": "2025-01-14T07:09:01.223699Z",
     "shell.execute_reply.started": "2025-01-14T07:09:01.109564Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "CdlhzCVNrfdO"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = IMG_DIR + 'dog.jpg'\n",
    "\n",
    "# 1. Load the image using PIL (same as Keras' image.load_img)\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# 2. Define the transformation to convert to a tensor (and resize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # Optional: Resize if needed\n",
    "    transforms.ToTensor()            # Converts to tensor and scales to [0, 1]\n",
    "])\n",
    "\n",
    "# 3. Apply the transformation\n",
    "img_tensor = transform(img)\n",
    "\n",
    "# 4. Add a batch dimension (if needed)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Now shape is (1, C, H, W)\n",
    "\n",
    "print(img_tensor.shape)\n",
    "print(img_tensor.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Prediction=model.forward(img_tensor.float().to(device))\n",
    "if Prediction >= .5:\n",
    "    print(\"DOG\")\n",
    "else:\n",
    "    print(\"CAT\")\n",
    "print(Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T07:09:01.961209Z",
     "iopub.status.busy": "2025-01-14T07:09:01.960463Z",
     "iopub.status.idle": "2025-01-14T07:09:02.052625Z",
     "shell.execute_reply": "2025-01-14T07:09:02.050884Z",
     "shell.execute_reply.started": "2025-01-14T07:09:01.961152Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "DQvNh2SgrfdO"
   },
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1737116921898,
     "user": {
      "displayName": "kc-aim",
      "userId": "16968997735368758960"
     },
     "user_tz": -480
    },
    "id": "ycagaYuMrfdO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1tUkLHIlAlxrtUi7tTuI3L6nI_a_S82GR",
     "timestamp": 1715836432373
    },
    {
     "file_id": "1Q0Qw29TdXwSTTuJYfu2f4RSlrmr9P8j5",
     "timestamp": 1712070397387
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "msds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
